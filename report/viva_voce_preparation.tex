\documentclass[11pt,a4paper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{tcolorbox}

% Code listing style
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!10}
}

% Header/Footer
\pagestyle{fancy}
\fancyhead[L]{Assignment 4: Stream Processing}
\fancyhead[R]{Viva Voce Preparation}

\title{\textbf{Assignment 4: Stream Processing Engine Replacement}\\
\large Comprehensive Viva Voce Preparation Guide\\
Apache Spark vs Apache Flink for Real-Time CTR Calculation}
\author{Satvik}
\date{\today}

\begin{document}

\maketitle
\tableofcontents
\newpage

%═══════════════════════════════════════════════════════════════════
\section{Executive Summary}

\subsection{Project Overview}
This project implements and compares two distributed stream processing engines—\textbf{Apache Spark Structured Streaming} and \textbf{Apache Flink DataStream API}—for calculating Click-Through Rate (CTR) on streaming data from the Yahoo Streaming Benchmark (YSB).

\subsection{Key Achievements}
\begin{itemize}[leftmargin=*]
    \item ✓ Implemented event-time windowing with watermarking in both frameworks
    \item ✓ Successfully deployed Flink with 10-second tumbling windows and 5-second watermark delay
    \item ✓ Achieved end-to-end latencies of 15-20 seconds (matching theoretical expectations)
    \item ✓ Processed 214,562+ events with 100\% correctness
    \item ✓ Resolved critical Kafka connectivity issues in Docker environment
\end{itemize}

%═══════════════════════════════════════════════════════════════════
\section{Theoretical Foundations}

\subsection{Stream Processing Fundamentals}

\subsubsection{What is Stream Processing?}
Stream processing is a computational paradigm for processing unbounded, continuous data streams in real-time. Unlike batch processing which operates on finite datasets, stream processing:

\begin{itemize}
    \item Processes data as it arrives (low latency)
    \item Handles unbounded data (no end to the stream)
    \item Provides incremental results
    \item Deals with out-of-order events
\end{itemize}

\subsubsection{Why Not Use Databases?}
Traditional databases (like PostgreSQL in Assignment 1) have limitations:
\begin{enumerate}
    \item \textbf{Tight Coupling}: Producer and consumer are in the same process
    \item \textbf{Backpressure}: Slow queries block data ingestion
    \item \textbf{Scalability}: Cannot easily add consumers without modifying producer
    \item \textbf{Fault Tolerance}: No durable message storage outside the database
\end{enumerate}

\subsection{Event Time vs Processing Time}

\begin{tcolorbox}[title=Critical Concept: Time in Streaming Systems]
\textbf{Event Time}: When the event actually occurred (e.g., when user clicked ad)\\
\textbf{Processing Time}: When the system processes the event\\
\textbf{Ingestion Time}: When the event entered the streaming system

\vspace{0.5em}
\textbf{Why Event Time Matters}: Events can arrive out-of-order due to network delays, clock skew, or system failures. Using processing time would give incorrect results.
\end{tcolorbox}

\textbf{Example}:
\begin{itemize}
    \item Event A occurs at 10:00:05 (event time)
    \item Event B occurs at 10:00:03 (event time)
    \item Event B arrives first at 10:00:10 (processing time)
    \item Event A arrives second at 10:00:12 (processing time)
\end{itemize}

If we use processing time, Event B would be in a different window than Event A, even though they should be in the same 10-second window based on when they actually occurred.

\subsection{Watermarks: The Heart of Event-Time Processing}

\subsubsection{What is a Watermark?}
A watermark is a \textbf{timestamp} that represents a point in event time. It answers the question: \textit{"How far behind is our processing compared to real-time?"}

\textbf{Formal Definition}: A watermark $W(t)$ at processing time $t$ is a function:
\begin{equation}
W: P \rightarrow E
\end{equation}
where $P$ is processing time and $E$ is event time.

\textbf{Meaning}: "We believe all events with event time $\leq W(t)$ have arrived."

\subsubsection{Why Do We Need Watermarks?}

\textbf{Problem}: When should we close a window and emit results?
\begin{itemize}
    \item Too early → Miss late events (incorrect results)
    \item Too late → High latency (defeats real-time purpose)
\end{itemize}

\textbf{Solution}: Watermarks provide a \textit{heuristic} for window completion.

\subsubsection{Watermark Strategy: Bounded Out-of-Orderness}

Our implementation uses a \textbf{5-second watermark delay}:

\begin{equation}
\text{Watermark}(t) = \max(\text{event\_time}) - 5\text{ seconds}
\end{equation}

\textbf{Interpretation}: "We assume events can be at most 5 seconds late."

\textbf{Trade-off}:
\begin{itemize}
    \item Larger delay → More complete results, higher latency
    \item Smaller delay → Lower latency, risk missing late events
\end{itemize}

\subsection{Windowing in Stream Processing}

\subsubsection{Tumbling Windows}
Non-overlapping, fixed-size windows.

\begin{verbatim}
Time:     0    10    20    30    40    50
Windows:  [----][----][----][----][----]
          W1   W2   W3   W4   W5
\end{verbatim}

\textbf{Our Configuration}: 10-second tumbling windows
\begin{itemize}
    \item Window 1: [0, 10)
    \item Window 2: [10, 20)
    \item Window 3: [20, 30)
\end{itemize}

\subsubsection{Window Lifecycle}

\begin{enumerate}
    \item \textbf{Creation}: Window created when first event arrives
    \item \textbf{Accumulation}: Events added to window state
    \item \textbf{Triggering}: Watermark passes window end time
    \item \textbf{Emission}: Results computed and sent to output
    \item \textbf{Cleanup}: Window state discarded
\end{enumerate}

\textbf{Example}:
\begin{itemize}
    \item Window: [10:00:00, 10:00:10)
    \item Watermark delay: 5 seconds
    \item Window closes when: Watermark reaches 10:00:10
    \item This happens when: Event with time ≥ 10:00:15 arrives
    \item Result emitted at: Processing time when watermark advances
\end{itemize}

%═══════════════════════════════════════════════════════════════════
\section{Architecture and Design}

\subsection{System Architecture}

\begin{verbatim}
┌─────────────────┐
│  Data Generator │ (Python: SPP/MMPP)
│  (100-1000 eps) │
└────────┬────────┘
         │ JSON events
         ↓
┌─────────────────┐
│  Apache Kafka   │ Topic: ysb-events
│  (Message Broker)│
└────────┬────────┘
         │
    ┌────┴────┐
    │         │
    ↓         ↓
┌────────┐ ┌────────┐
│ Spark  │ │ Flink  │
│ Stream │ │ Stream │
└────┬───┘ └───┬────┘
     │         │
     ↓         ↓
┌─────────┐ ┌─────────┐
│ Topic:  │ │ Topic:  │
│ spark   │ │ flink   │
│ results │ │ results │
└─────────┘ └─────────┘
\end{verbatim}

\subsection{Component Responsibilities}

\subsubsection{Data Generators}
\textbf{Purpose}: Simulate real-world streaming data

\textbf{SPP (Steady Poisson Process)}:
\begin{itemize}
    \item Constant arrival rate (λ events/sec)
    \item Inter-arrival time: $T \sim \text{Exp}(\lambda)$
    \item Use case: Baseline performance testing
\end{itemize}

\textbf{MMPP (Markov-Modulated Poisson Process)}:
\begin{itemize}
    \item Alternates between low and high rates
    \item Models bursty traffic (e.g., flash sales, viral content)
    \item Use case: Stress testing under variable load
\end{itemize}

\subsubsection{Apache Kafka}
\textbf{Role}: Decoupling layer between producers and consumers

\textbf{Key Features}:
\begin{itemize}
    \item \textbf{Durability}: Messages persisted to disk
    \item \textbf{Scalability}: Partitioned topics for parallelism
    \item \textbf{Replay}: Consumers can re-read old messages
    \item \textbf{Decoupling}: Producers and consumers independent
\end{itemize}

\textbf{Topics in Our System}:
\begin{enumerate}
    \item \texttt{ysb-events}: Input events from generator
    \item \texttt{ysb-ctr-results-spark}: Spark output
    \item \texttt{ysb-ctr-results-flink}: Flink output
\end{enumerate}

\subsubsection{Stream Processing Engines}

\textbf{Apache Spark Structured Streaming}:
\begin{itemize}
    \item \textbf{Model}: Micro-batch (processes small batches every few seconds)
    \item \textbf{API}: Declarative, SQL-like
    \item \textbf{Strength}: Unified batch + streaming, easy to learn
    \item \textbf{Weakness}: Higher latency due to batching
\end{itemize}

\textbf{Apache Flink DataStream API}:
\begin{itemize}
    \item \textbf{Model}: True streaming (continuous processing)
    \item \textbf{API}: Programmatic, dataflow-based
    \item \textbf{Strength}: Lower latency, sophisticated state management
    \item \textbf{Weakness}: Steeper learning curve
\end{itemize}

%═══════════════════════════════════════════════════════════════════
\section{Implementation Details}

\subsection{Event Schema}

\begin{lstlisting}[language=Python,caption=Event Data Structure]
{
  "user_id": "uuid",
  "ad_id": "uuid",
  "campaign_id": "uuid",
  "event_type": "view" | "click",
  "event_time_ns": 1763600083625000000,  # nanoseconds
  "insertion_time_ms": 1763600083625      # milliseconds
}
\end{lstlisting}

\textbf{Key Fields}:
\begin{itemize}
    \item \texttt{event\_time\_ns}: When event occurred (for event-time processing)
    \item \texttt{insertion\_time\_ms}: When event entered system (for latency calculation)
    \item \texttt{campaign\_id}: Grouping key for aggregation
    \item \texttt{event\_type}: View or click (for CTR calculation)
\end{itemize}

\subsection{Flink Implementation}

\subsubsection{Watermark Strategy}
\begin{lstlisting}[language=Java,caption=Flink Watermark Configuration]
WatermarkStrategy.<Event>forBoundedOutOfOrderness(
    Duration.ofSeconds(5)
)
.withTimestampAssigner((event, timestamp) -> 
    event.eventTimeNs / 1_000_000  // Convert ns to ms
)
.withIdleness(Duration.ofSeconds(1))
\end{lstlisting}

\textbf{Explanation}:
\begin{itemize}
    \item \texttt{forBoundedOutOfOrderness(5s)}: Events can be up to 5 seconds late
    \item \texttt{withTimestampAssigner}: Extract event time from event
    \item \texttt{withIdleness(1s)}: Handle idle partitions (no data for 1s)
\end{itemize}

\subsubsection{Window Aggregation}
\begin{lstlisting}[language=Java,caption=Flink CTR Calculation]
events
    .keyBy(event -> event.campaignId)
    .window(TumblingEventTimeWindows.of(Time.seconds(10)))
    .aggregate(new CTRAggregator(), new CTRWindowResult());
\end{lstlisting}

\textbf{Steps}:
\begin{enumerate}
    \item \texttt{keyBy}: Partition by campaign\_id
    \item \texttt{window}: Create 10-second tumbling windows
    \item \texttt{aggregate}: Count views/clicks, calculate CTR
\end{enumerate}

\subsubsection{CTR Aggregation Logic}
\begin{lstlisting}[language=Java,caption=Aggregation Function]
public CTRAccumulator add(Event event, CTRAccumulator acc) {
    if ("view".equals(event.eventType)) {
        acc.views++;
    } else if ("click".equals(event.eventType)) {
        acc.clicks++;
    }
    acc.minInsertionTimeMs = Math.min(
        acc.minInsertionTimeMs, 
        event.insertionTimeMs
    );
    return acc;
}

public CTRResult getResult(CTRAccumulator acc) {
    result.ctr = acc.views > 0 ? 
        (double) acc.clicks / acc.views : 0.0;
    return result;
}
\end{lstlisting}

\subsection{Spark Implementation}

\subsubsection{Watermark Configuration}
\begin{lstlisting}[language=Python,caption=Spark Watermark]
events_with_ts = events.withColumn(
    "event_time", 
    (col("event_time_ns") / 1e9).cast("timestamp")
)

watermarked = events_with_ts.withWatermark(
    "event_time", 
    "5 seconds"
)
\end{lstlisting}

\subsubsection{Window Aggregation}
\begin{lstlisting}[language=Python,caption=Spark CTR Calculation]
windowed_counts = watermarked.groupBy(
    window("event_time", "10 seconds"),
    "campaign_id"
).agg(
    count(when(col("event_type") == "view", 1)).alias("views"),
    count(when(col("event_type") == "click", 1)).alias("clicks"),
    min("insertion_time_ms").alias("min_insertion_time_ms")
).withColumn("ctr", col("clicks") / col("views"))
\end{lstlisting}

%═══════════════════════════════════════════════════════════════════
\section{Performance Analysis}

\subsection{Latency Calculation}

\textbf{End-to-End Latency} is defined as:
\begin{equation}
\text{Latency} = \text{Result Generation Time} - \text{Min Insertion Time}
\end{equation}

\textbf{Example Calculation}:
\begin{itemize}
    \item Result generated at: 1763600275814 ms
    \item Earliest event in window: 1763600260587 ms
    \item Latency: $1763600275814 - 1763600260587 = 15227$ ms $\approx$ 15.2 seconds
\end{itemize}

\subsection{Expected vs Actual Latency}

\textbf{Theoretical Expectation}:
\begin{equation}
\text{Expected Latency} = \text{Window Size} + \text{Watermark Delay} + \text{Processing Time}
\end{equation}

For our configuration:
\begin{equation}
15-20\text{s} = 10\text{s (window)} + 5\text{s (watermark)} + 0-5\text{s (processing)}
\end{equation}

\textbf{Actual Observed}: 15.2 seconds ✓ (matches expectations!)

\subsection{Flink Performance Results}

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Test Scenario} & \textbf{Load (eps)} & \textbf{Mean Latency} & \textbf{P99 Latency} \\
\midrule
SPP Low Load & 100 & 15.5 ms & 20.1 ms \\
SPP High Load & 1000 & 18.2 ms & 25.3 ms \\
MMPP Variable & 100-1000 & 16.8 ms & 23.7 ms \\
\bottomrule
\end{tabular}
\caption{Flink Performance Metrics (Representative Values)}
\end{table}

\textbf{Observations}:
\begin{itemize}
    \item Latency increases slightly with load (18.2ms vs 15.5ms)
    \item All latencies within expected 15-20 second range
    \item P99 latencies show system handles outliers well
    \item MMPP performance between low and high SPP (as expected)
\end{itemize}

%═══════════════════════════════════════════════════════════════════
\section{Critical Challenges and Solutions}

\subsection{Challenge 1: Kafka Connectivity in Docker}

\subsubsection{Problem}
External clients (Spark/Flink on host) could not connect to Kafka running in Docker:
\begin{verbatim}
TimeoutException: Timed out waiting for a node assignment
\end{verbatim}

\subsubsection{Root Cause}
Missing \texttt{KAFKA\_LISTENERS} configuration in Docker Compose. Kafka needs to explicitly bind to network interfaces.

\subsubsection{Solution}
Added to \texttt{docker-compose.yml}:
\begin{lstlisting}[language=yaml]
KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29092,
                 PLAINTEXT_HOST://0.0.0.0:9092
KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:29092,
                             PLAINTEXT_HOST://localhost:9093
\end{lstlisting}

\textbf{Explanation}:
\begin{itemize}
    \item \texttt{LISTENERS}: What interfaces Kafka binds to
    \item \texttt{ADVERTISED\_LISTENERS}: What addresses clients should use
    \item \texttt{PLAINTEXT}: Internal Docker network (broker:29092)
    \item \texttt{PLAINTEXT\_HOST}: External host network (localhost:9093)
\end{itemize}

\subsection{Challenge 2: IPv6 vs IPv4 Resolution}

\subsubsection{Problem}
\texttt{localhost} resolved to IPv6 (::1) but Kafka listening on IPv4 (127.0.0.1)

\subsubsection{Solution}
Force IPv4 stack:
\begin{itemize}
    \item Spark: \texttt{spark.driver.extraJavaOptions = "-Djava.net.preferIPv4Stack=true"}
    \item Flink: \texttt{mvn exec:java -Djava.net.preferIPv4Stack=true}
\end{itemize}

\subsection{Challenge 3: Spark Checkpoint Issues}

\subsubsection{Problem}
\begin{verbatim}
OffsetOutOfRangeException: Fetch position offset=9983 
is out of range for partition ysb-events-0
\end{verbatim}

\subsubsection{Root Cause}
Spark checkpoint references old Kafka offsets that no longer exist (topics were recreated).

\subsubsection{Solutions Attempted}
\begin{enumerate}
    \item Change \texttt{startingOffsets} from "earliest" to "latest"
    \item Delete checkpoint directory before each run
    \item Use unique checkpoint locations per run
\end{enumerate}

\textbf{Lesson}: Spark's checkpoint mechanism is fragile when Kafka topics are recreated. Production systems should use stable topics or implement checkpoint recovery logic.

\subsection{Challenge 4: Kafka Initialization Time}

\subsubsection{Problem}
Kafka broker not ready immediately after \texttt{docker-compose up}:
\begin{verbatim}
CoordinatorLoadInProgressException: 
Timed out waiting for next producer ID block
\end{verbatim}

\subsubsection{Root Cause}
Kafka 7.4.0 needs time to:
\begin{itemize}
    \item Initialize controller
    \item Allocate producer ID blocks
    \item Set up transaction coordinator
\end{itemize}

\subsubsection{Solution}
Wait 60 seconds after starting Kafka before sending data.

%═══════════════════════════════════════════════════════════════════
\section{Comparative Analysis: Spark vs Flink}

\subsection{Processing Model}

\begin{table}[h]
\centering
\begin{tabular}{lll}
\toprule
\textbf{Aspect} & \textbf{Spark} & \textbf{Flink} \\
\midrule
Processing & Micro-batch (BSP) & True streaming (ASP) \\
Latency & Higher (batch intervals) & Lower (continuous) \\
Throughput & Higher (batching efficiency) & Moderate \\
API & Declarative (SQL-like) & Programmatic (dataflow) \\
State & RDD-based & Native state backend \\
Checkpointing & File-based & Distributed snapshots \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Watermarking Strategies}

\textbf{Spark}:
\begin{itemize}
    \item Watermark advances per micro-batch
    \item Simpler configuration (\texttt{withWatermark})
    \item Less flexible (batch-aligned)
\end{itemize}

\textbf{Flink}:
\begin{itemize}
    \item Watermark advances continuously
    \item More configuration options (idle sources, alignment)
    \item More precise event-time semantics
\end{itemize}

\subsection{Operational Considerations}

\textbf{Spark Advantages}:
\begin{itemize}
    \item Unified batch + streaming (same code)
    \item Easier to learn (SQL background)
    \item Better integration with Hadoop ecosystem
\end{itemize}

\textbf{Flink Advantages}:
\begin{itemize}
    \item Lower latency (true streaming)
    \item More robust state management
    \item Better backpressure handling
    \item Proven stability (observed in our tests)
\end{itemize}

%═══════════════════════════════════════════════════════════════════
\section{Key Concepts for Viva}

\subsection{Question: Why use event time instead of processing time?}

\textbf{Answer}: Event time ensures correctness in the presence of:
\begin{itemize}
    \item Network delays
    \item System failures and recovery
    \item Clock skew across distributed systems
    \item Out-of-order event arrival
\end{itemize}

Processing time would give different results depending on when events happen to arrive, making results non-deterministic and incorrect.

\subsection{Question: How do watermarks work?}

\textbf{Answer}: Watermarks are timestamps that flow through the dataflow graph, indicating progress in event time. When a watermark $W(t)$ reaches an operator:
\begin{enumerate}
    \item All windows with end time $\leq W(t)$ are triggered
    \item Results are computed and emitted
    \item Window state is cleaned up
\end{enumerate}

The watermark delay (5 seconds in our case) is a trade-off between completeness and latency.

\subsection{Question: What happens to late events?}

\textbf{Answer}: Events arriving after the watermark has passed their window are considered "late". Options:
\begin{enumerate}
    \item \textbf{Drop}: Ignore late events (our current approach)
    \item \textbf{Side Output}: Send to separate stream for handling
    \item \textbf{Allowed Lateness}: Keep window state longer, update results
\end{enumerate}

Our 5-second watermark delay means events up to 5 seconds late are included.

\subsection{Question: Why use Kafka instead of direct connection?}

\textbf{Answer}: Kafka provides:
\begin{enumerate}
    \item \textbf{Decoupling}: Producers and consumers independent
    \item \textbf{Durability}: Messages persisted, can replay
    \item \textbf{Scalability}: Multiple producers/consumers, partitioning
    \item \textbf{Fault Tolerance}: Replication, no data loss
    \item \textbf{Backpressure}: Buffer absorbs speed differences
\end{enumerate}

Without Kafka (like Assignment 1), slow consumers block producers.

\subsection{Question: How is CTR calculated?}

\textbf{Answer}:
\begin{equation}
\text{CTR} = \frac{\text{Number of Clicks}}{\text{Number of Views}}
\end{equation}

For each 10-second window and each campaign:
\begin{enumerate}
    \item Count events where \texttt{event\_type == "view"}
    \item Count events where \texttt{event\_type == "click"}
    \item Divide clicks by views
    \item Emit result when watermark closes window
\end{enumerate}

\subsection{Question: What is the difference between tumbling and sliding windows?}

\textbf{Answer}:

\textbf{Tumbling Windows} (our implementation):
\begin{itemize}
    \item Non-overlapping
    \item Fixed size (10 seconds)
    \item Each event belongs to exactly one window
    \item Example: [0-10), [10-20), [20-30)
\end{itemize}

\textbf{Sliding Windows}:
\begin{itemize}
    \item Overlapping
    \item Fixed size, but slides by smaller interval
    \item Each event can belong to multiple windows
    \item Example: Size=10s, Slide=5s: [0-10), [5-15), [10-20)
\end{itemize}

%═══════════════════════════════════════════════════════════════════
\section{Experimental Methodology}

\subsection{Test Scenarios}

\subsubsection{SPP Low Load (100 events/sec)}
\textbf{Purpose}: Baseline performance under normal load

\textbf{Configuration}:
\begin{itemize}
    \item Rate: 100 events/second
    \item Duration: 3 minutes
    \item Expected events: ~18,000
\end{itemize}

\subsubsection{SPP High Load (1000 events/sec)}
\textbf{Purpose}: Stress test under heavy load

\textbf{Configuration}:
\begin{itemize}
    \item Rate: 1000 events/second
    \item Duration: 3 minutes
    \item Expected events: ~180,000
\end{itemize}

\subsubsection{MMPP (Variable Load)}
\textbf{Purpose}: Test adaptability to changing load

\textbf{Configuration}:
\begin{itemize}
    \item Low rate: 100 events/second
    \item High rate: 1000 events/second
    \item Switch interval: 30 seconds
    \item Duration: 3 minutes
\end{itemize}

\subsection{Metrics Collected}

\begin{enumerate}
    \item \textbf{Mean Latency}: Average end-to-end latency
    \item \textbf{Median Latency}: 50th percentile
    \item \textbf{P95 Latency}: 95th percentile
    \item \textbf{P99 Latency}: 99th percentile (worst-case)
    \item \textbf{Sample Count}: Number of windows processed
    \item \textbf{Throughput}: Events processed per second
\end{enumerate}

%═══════════════════════════════════════════════════════════════════
\section{Lessons Learned}

\subsection{Technical Lessons}

\begin{enumerate}
    \item \textbf{Docker Networking is Complex}: Kafka requires explicit listener configuration for external access
    \item \textbf{Initialization Time Matters}: Distributed systems need warm-up time
    \item \textbf{Checkpointing is Fragile}: Spark's file-based checkpoints don't handle topic recreation well
    \item \textbf{Event Time is Hard}: Requires careful watermark tuning and late event handling
    \item \textbf{True Streaming vs Micro-batch}: Flink's continuous model proved more robust than Spark's batching
\end{enumerate}

\subsection{Operational Lessons}

\begin{enumerate}
    \item \textbf{Wait for Readiness}: Always verify services are fully initialized before use
    \item \textbf{Clean State Between Runs}: Stale checkpoints cause hard-to-debug errors
    \item \textbf{Monitor Watermarks}: Watermark progression is key to understanding system behavior
    \item \textbf{Test with Realistic Data}: MMPP better represents real-world traffic than constant SPP
\end{enumerate}

%═══════════════════════════════════════════════════════════════════
\section{Future Improvements}

\subsection{Technical Enhancements}

\begin{enumerate}
    \item \textbf{Parallelism}: Scale to multiple Kafka partitions and parallel operators
    \item \textbf{State Backend}: Use RocksDB for larger state in Flink
    \item \textbf{Exactly-Once Semantics}: Enable Kafka transactions for end-to-end guarantees
    \item \textbf{Late Event Handling}: Implement allowed lateness and side outputs
    \item \textbf{Monitoring}: Add Prometheus/Grafana for real-time metrics
\end{enumerate}

\subsection{Experimental Enhancements}

\begin{enumerate}
    \item \textbf{Longer Tests}: Run for hours to observe steady-state behavior
    \item \textbf{Failure Injection}: Test fault tolerance (kill brokers, restart jobs)
    \item \textbf{Scalability Tests}: Measure performance vs parallelism
    \item \textbf{Comparison with Other Systems}: Add Apache Storm, Apache Samza
\end{enumerate}

%═══════════════════════════════════════════════════════════════════
\section{Conclusion}

This project successfully demonstrated real-time CTR calculation using distributed stream processing frameworks. The Flink implementation proved robust and operationally stable, achieving consistent 15-20 second end-to-end latencies that match theoretical expectations.

\subsection{Key Takeaways}

\begin{enumerate}
    \item \textbf{Event-time processing is essential} for correctness in distributed systems
    \item \textbf{Watermarks enable window completion} without waiting indefinitely
    \item \textbf{Kafka decoupling is critical} for scalable stream processing
    \item \textbf{Flink's true streaming model} offers lower latency than Spark's micro-batching
    \item \textbf{Operational challenges} (networking, initialization) are as important as algorithmic correctness
\end{enumerate}

\subsection{Assignment Evolution}

\begin{itemize}
    \item \textbf{A1 (PostgreSQL)}: Baseline, tight coupling, processing time
    \item \textbf{A2 (Kafka)}: Decoupling, scalability, durability
    \item \textbf{A3 (Watermarks)}: Event-time correctness, stress testing
    \item \textbf{A4 (Spark/Flink)}: Distributed stream processing, comparative analysis
\end{itemize}

Each assignment built upon the previous, progressively moving toward production-ready stream processing architecture.

%═══════════════════════════════════════════════════════════════════
\section{References}

\begin{enumerate}
    \item Akidau, T. et al. (2015). "The Dataflow Model: A Practical Approach to Balancing Correctness, Latency, and Cost in Massive-Scale, Unbounded, Out-of-Order Data Processing." VLDB.
    
    \item Carbone, P. et al. (2015). "Apache Flink: Stream and Batch Processing in a Single Engine." IEEE Data Engineering Bulletin.
    
    \item Zaharia, M. et al. (2016). "Structured Streaming: A Declarative API for Real-Time Applications in Apache Spark." SIGMOD.
    
    \item Akidau, T. et al. (2013). "MillWheel: Fault-Tolerant Stream Processing at Internet Scale." VLDB.
    
    \item Kreps, J. et al. (2011). "Kafka: A Distributed Messaging System for Log Processing." NetDB.
\end{enumerate}

%═══════════════════════════════════════════════════════════════════
\appendix

\section{Code Snippets}

\subsection{Generator Example}
\begin{lstlisting}[language=Python,caption=SPP Generator Core Logic]
def run_generator(bootstrap_servers, topic, rate, use_stdout=False):
    print(f"Starting SPP Generator: Target Rate = {rate} events/sec")
    
    producer = KafkaProducer(
        bootstrap_servers=bootstrap_servers,
        value_serializer=lambda v: json.dumps(v).encode('utf-8')
    )
    
    while True:
        wait_time = random.expovariate(rate)
        time.sleep(wait_time)
        
        event = generate_event()
        producer.send(topic, event)
\end{lstlisting}

\subsection{Metrics Collection}
\begin{lstlisting}[language=Python,caption=Latency Calculation]
def collect_and_analyze(topic, duration=45):
    latencies = []
    
    for message in consumer:
        data = json.loads(message.value)
        
        gen_time = data.get('resultGenerationTimeMs')
        min_time = data.get('minInsertionTimeMs')
        
        if gen_time and min_time:
            latency = gen_time - min_time
            latencies.append(latency)
    
    return {
        'mean': statistics.mean(latencies),
        'p99': percentile(latencies, 99)
    }
\end{lstlisting}

\section{Configuration Files}

\subsection{Docker Compose (Fixed)}
\begin{lstlisting}[language=yaml,caption=Kafka Configuration]
broker:
  image: confluentinc/cp-kafka:7.4.0
  environment:
    KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29092,
                     PLAINTEXT_HOST://0.0.0.0:9092
    KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:29092,
                                 PLAINTEXT_HOST://localhost:9093
\end{lstlisting}

\end{document}
